"""
æ¨¡å‹æœåŠ¡
å°è£…AIæ¨¡å‹çš„é€‰æ‹©ã€åˆ‡æ¢ã€çŠ¶æ€ç®¡ç†ç­‰é€»è¾‘
é›†æˆåŸºç¡€è®¾æ–½å±‚çš„é…ç½®å’Œæ—¥å¿—æœåŠ¡
"""

from typing import List, Dict, Optional, Tuple
from datetime import datetime

from src.shared.state.application_state import app_state
from src.infrastructure import (
    IConfigurationService,
    ILoggingService,
    get_config,
    get_logger,
    performance_monitor
)


class ModelService:
    """æ¨¡å‹ç®¡ç†æœåŠ¡"""

    def __init__(self, config_service: IConfigurationService = None, logging_service: ILoggingService = None):
        # åŸºç¡€è®¾æ–½æœåŠ¡
        self._config_service = config_service or get_config()
        self._logger = logging_service or get_logger()

        # ä»é…ç½®è·å–æ¨¡å‹å…ƒæ•°æ®
        self._model_metadata = self._config_service.get_value("model_metadata", {
            "gemini-2.5-flash-preview-05-20": {
                "display_name": "Gemini 2.5 Flash Preview",
                "description": "æœ€æ–°é¢„è§ˆç‰ˆæœ¬ï¼Œé€Ÿåº¦å¿«ï¼Œé€‚åˆæ—¥å¸¸é—®ç­”",
                "max_tokens": 8192,
                "recommended": True
            },
            "gemini-2.0-flash": {
                "display_name": "Gemini 2.0 Flash",
                "description": "ç¨³å®šç‰ˆæœ¬ï¼Œå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡",
                "max_tokens": 8192,
                "recommended": True
            },
            "gemini-2.0-flash-lite": {
                "display_name": "Gemini 2.0 Flash Lite",
                "description": "è½»é‡ç‰ˆæœ¬ï¼Œå“åº”æ›´å¿«",
                "max_tokens": 4096,
                "recommended": False
            },
            "gemini-1.5-flash": {
                "display_name": "Gemini 1.5 Flash",
                "description": "ç»å…¸å¿«é€Ÿç‰ˆæœ¬",
                "max_tokens": 8192,
                "recommended": False
            },
            "gemini-1.5-pro": {
                "display_name": "Gemini 1.5 Pro",
                "description": "é«˜è´¨é‡ç‰ˆæœ¬ï¼Œé€‚åˆå¤æ‚é—®ç­”",
                "max_tokens": 32768,
                "recommended": True
            }
        })

        self._logger.info("ModelService åˆå§‹åŒ–å®Œæˆ", extra={
            "available_models_count": len(self._model_metadata)
        })

    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return app_state.available_models

    def get_current_model(self) -> str:
        """è·å–å½“å‰æ¨¡å‹"""
        return app_state.current_model

    @performance_monitor(get_logger())
    def switch_model(self, model_name: str) -> Tuple[bool, str]:
        """
        åˆ‡æ¢æ¨¡å‹

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            (æ˜¯å¦æˆåŠŸ, çŠ¶æ€æ¶ˆæ¯)
        """
        try:
            if model_name not in self.get_available_models():
                self._logger.warning("å°è¯•åˆ‡æ¢åˆ°ä¸æ”¯æŒçš„æ¨¡å‹", extra={
                    "requested_model": model_name,
                    "available_models": self.get_available_models()
                })
                return False, f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}"

            old_model = app_state.current_model
            app_state.current_model = model_name

            # é‡ç½®QAé“¾ï¼Œå¼ºåˆ¶ä½¿ç”¨æ–°æ¨¡å‹é‡æ–°åˆå§‹åŒ–
            app_state.qa_chain = None

            success_msg = f"âœ… æ¨¡å‹å·²åˆ‡æ¢: {old_model} â†’ {model_name}"
            self._logger.info("æ¨¡å‹åˆ‡æ¢æˆåŠŸ", extra={
                "old_model": old_model,
                "new_model": model_name
            })

            return True, success_msg

        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹åˆ‡æ¢å¤±è´¥: {str(e)}"
            self._logger.error("æ¨¡å‹åˆ‡æ¢å¤±è´¥", exception=e, extra={
                "requested_model": model_name
            })
            return False, error_msg

    def get_model_status(self) -> str:
        """è·å–å½“å‰æ¨¡å‹çŠ¶æ€"""
        try:
            current_model = app_state.current_model
            model_info = self._model_metadata.get(current_model, {})

            status_parts = [
                f"ğŸ¤– å½“å‰æ¨¡å‹: {model_info.get('display_name', current_model)}",
                f"ğŸ“ æè¿°: {model_info.get('description', 'æ ‡å‡†AIæ¨¡å‹')}",
                f"ğŸ“Š æœ€å¤§token: {model_info.get('max_tokens', 'æœªçŸ¥')}",
                f"â­ æ¨è: {'æ˜¯' if model_info.get('recommended', False) else 'å¦'}",
                f"ğŸ”§ çŠ¶æ€: {'å°±ç»ª' if app_state.qa_chain else 'éœ€è¦åˆå§‹åŒ–'}"
            ]

            return "\\n".join(status_parts)

        except Exception as e:
            return f"âŒ è·å–æ¨¡å‹çŠ¶æ€å¤±è´¥: {str(e)}"

    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """è·å–æŒ‡å®šæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯"""
        return self._model_metadata.get(model_name)

    def get_recommended_models(self) -> List[str]:
        """è·å–æ¨èæ¨¡å‹åˆ—è¡¨"""
        return [
            model for model, info in self._model_metadata.items()
            if info.get('recommended', False) and model in self.get_available_models()
        ]

    def validate_model_compatibility(self, model_name: str) -> Tuple[bool, str]:
        """
        éªŒè¯æ¨¡å‹å…¼å®¹æ€§

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            (æ˜¯å¦å…¼å®¹, éªŒè¯æ¶ˆæ¯)
        """
        try:
            if model_name not in self.get_available_models():
                return False, f"æ¨¡å‹ {model_name} ä¸åœ¨æ”¯æŒåˆ—è¡¨ä¸­"

            model_info = self._model_metadata.get(model_name)
            if not model_info:
                return False, f"æ¨¡å‹ {model_name} ç¼ºå°‘å…ƒæ•°æ®ä¿¡æ¯"

            # æ£€æŸ¥tokené™åˆ¶
            max_tokens = model_info.get('max_tokens', 0)
            if max_tokens < 1000:
                return False, f"æ¨¡å‹ {model_name} tokené™åˆ¶è¿‡ä½ ({max_tokens})"

            return True, f"æ¨¡å‹ {model_name} å…¼å®¹æ€§éªŒè¯é€šè¿‡"

        except Exception as e:
            return False, f"éªŒè¯æ¨¡å‹å…¼å®¹æ€§æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"

    def get_model_selection_info(self) -> str:
        """è·å–æ¨¡å‹é€‰æ‹©ä¿¡æ¯"""
        try:
            available_models = self.get_available_models()
            current_model = self.get_current_model()

            info_parts = ["## ğŸ¤– å¯ç”¨æ¨¡å‹ä¿¡æ¯\\n"]

            for model in available_models:
                model_info = self._model_metadata.get(model, {})
                is_current = " **(å½“å‰)**" if model == current_model else ""
                is_recommended = " ğŸŒŸ" if model_info.get('recommended', False) else ""

                info_parts.append(
                    f"**{model_info.get('display_name', model)}{is_current}{is_recommended}**\\n"
                    f"- æè¿°: {model_info.get('description', 'æ ‡å‡†AIæ¨¡å‹')}\\n"
                    f"- æœ€å¤§token: {model_info.get('max_tokens', 'æœªçŸ¥')}\\n"
                )

            info_parts.append("\\nğŸŒŸ = æ¨èæ¨¡å‹")

            return "\\n".join(info_parts)

        except Exception as e:
            return f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}"

    def force_model_reinit(self) -> str:
        """å¼ºåˆ¶æ¨¡å‹é‡æ–°åˆå§‹åŒ–"""
        try:
            app_state.qa_chain = None
            app_state.system_ready = False

            return f"âœ… æ¨¡å‹ {app_state.current_model} å·²é‡æ–°åˆå§‹åŒ–"

        except Exception as e:
            return f"âŒ æ¨¡å‹é‡æ–°åˆå§‹åŒ–å¤±è´¥: {str(e)}"