"""
æ–‡æ¡£å¤„ç†æœåŠ¡
å°è£…å¤šç§æ–‡æ¡£æ ¼å¼çš„ä¸Šä¼ ã€å¤„ç†ã€å‘é‡åŒ–ç­‰æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
æ”¯æŒPDFã€Wordã€Excelã€PowerPointã€Markdownã€æ–‡æœ¬ç­‰æ ¼å¼
"""

import os
import tempfile
import shutil
from pathlib import Path
from typing import Optional, Tuple, List
from datetime import datetime
import io
import base64
from PIL import Image

# å¯¼å…¥å„ç§æ–‡æ¡£åŠ è½½å™¨
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredExcelLoader,
    UnstructuredPowerPointLoader,
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.schema import Document

# Google Gemini Vision API
import google.generativeai as genai

# Officeæ–‡æ¡£å¤„ç†
from docx import Document as DocxDocument
import pandas as pd
from pptx import Presentation

# ä½¿ç”¨æ–°çš„åŸºç¡€è®¾æ–½æœåŠ¡
from src.infrastructure.config.config_migration_adapter import get_legacy_config
from src.infrastructure.utilities import get_utility_service
from src.infrastructure import get_logger
from src.shared.state.application_state import app_state, FileInfo


class DocumentService:
    """æ–‡æ¡£å¤„ç†æœåŠ¡ - æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼"""

    def __init__(self, model_service=None, config_service=None, logger_service=None, utility_service=None):
        """åˆå§‹åŒ–æ–‡æ¡£æœåŠ¡

        Args:
            model_service: æ¨¡å‹ç®¡ç†æœåŠ¡å®ä¾‹ï¼Œç”¨äºä¾èµ–æ³¨å…¥
            config_service: é…ç½®æœåŠ¡å®ä¾‹
            logger_service: æ—¥å¿—æœåŠ¡å®ä¾‹
            utility_service: å·¥å…·æœåŠ¡å®ä¾‹
        """
        self.model_service = model_service

        # è·å–æœåŠ¡å®ä¾‹ (æ”¯æŒä¾èµ–æ³¨å…¥)
        if config_service:
            # å¦‚æœæä¾›äº†ConfigurationServiceï¼Œä½¿ç”¨ConfigMigrationAdapteré€‚é…
            from src.infrastructure.config.config_migration_adapter import ConfigMigrationAdapter
            self.config = ConfigMigrationAdapter(config_service)
        else:
            self.config = get_legacy_config()
        self.logger = logger_service or get_logger()
        self.utility_service = utility_service or get_utility_service()

        # æ”¯æŒçš„æ–‡æ¡£æ ¼å¼
        self.supported_formats = [".pdf", ".docx", ".xlsx", ".pptx", ".txt", ".md"]

        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆæ”¯æŒè¯­ä¹‰åˆ†å—ï¼‰
        self._init_text_splitters()

        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=self.config.GOOGLE_API_KEY
        )

        # åˆå§‹åŒ–Google Gemini Vision API
        genai.configure(api_key=self.config.GOOGLE_API_KEY)
        self.vision_model = genai.GenerativeModel('gemini-1.5-flash')

        self.logger.info("DocumentService åˆå§‹åŒ–å®Œæˆ", extra={
            "chunk_size": self.config.CHUNK_SIZE,
            "chunk_overlap": self.config.CHUNK_OVERLAP,
            "use_semantic_chunking": self.config.USE_SEMANTIC_CHUNKING,
            "supported_formats": self.supported_formats
        })

    def _init_text_splitters(self):
        """åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨"""
        # ä¼ ç»Ÿåˆ†å—å™¨ï¼ˆå¤‡ç”¨ï¼‰
        self.traditional_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.CHUNK_SIZE,
            chunk_overlap=self.config.CHUNK_OVERLAP,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )

        # è¯­ä¹‰åˆ†å—å™¨
        try:
            from .semantic_text_splitter import AdaptiveSemanticSplitter
            self.semantic_splitter = AdaptiveSemanticSplitter(config=self.config)
            self.logger.info("è¯­ä¹‰åˆ†å—å™¨åˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            self.logger.warning(f"è¯­ä¹‰åˆ†å—å™¨å¯¼å…¥å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿåˆ†å—å™¨")
            self.semantic_splitter = None
        except Exception as e:
            self.logger.error(f"è¯­ä¹‰åˆ†å—å™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿåˆ†å—å™¨")
            self.semantic_splitter = None

        # æ ¹æ®é…ç½®é€‰æ‹©é»˜è®¤åˆ†å—å™¨
        use_semantic = self.config.USE_SEMANTIC_CHUNKING
        if use_semantic and self.semantic_splitter:
            self.text_splitter = self.semantic_splitter
            self.logger.info("é»˜è®¤ä½¿ç”¨è¯­ä¹‰åˆ†å—å™¨")
        else:
            self.text_splitter = self.traditional_splitter
            self.logger.info("é»˜è®¤ä½¿ç”¨ä¼ ç»Ÿåˆ†å—å™¨")

    def process_document(self, file) -> str:
        """
        å¤„ç†æ–‡æ¡£æ–‡ä»¶å¹¶åˆ›å»ºå‘é‡æ•°æ®åº“
        æ”¯æŒPDFã€Wordã€Excelã€PowerPointã€Markdownã€æ–‡æœ¬ç­‰æ ¼å¼

        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡

        Returns:
            å¤„ç†ç»“æœæ¶ˆæ¯
        """
        self.logger.info("å¼€å§‹å¤„ç†æ–‡æ¡£æ–‡ä»¶", extra={"file": str(file)})

        if file is None:
            self.logger.warning("æœªæä¾›æ–‡ä»¶")
            return "âŒ è¯·é€‰æ‹©ä¸€ä¸ªæ–‡æ¡£æ–‡ä»¶"

        try:
            # è·å–æ–‡ä»¶è·¯å¾„å’Œåç§°
            file_path, file_name = self._get_file_info(file)

            self.logger.info("è·å–æ–‡ä»¶ä¿¡æ¯", extra={
                "file_path": file_path,
                "file_name": file_name
            })

            # éªŒè¯æ–‡ä»¶
            if not self._validate_file(file_path):
                return f"âŒ æ–‡ä»¶éªŒè¯å¤±è´¥: {file_path}"

            self.logger.info("æ–‡ä»¶éªŒè¯é€šè¿‡ï¼Œå¼€å§‹å¤„ç†æ–‡æ¡£")

            # åŠ è½½å¹¶å¤„ç†æ–‡æ¡£
            documents = self._load_document(file_path)
            if not documents:
                return "âŒ æ–‡æ¡£æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–"

            self.logger.info("æˆåŠŸåŠ è½½æ–‡æ¡£", extra={"pages": len(documents)})

            # åˆ†å‰²æ–‡æ¡£
            texts = self._split_documents(documents)
            self.logger.info("æ–‡æ¡£åˆ†å‰²å®Œæˆ", extra={"chunks": len(texts)})

            # æ£€æŸ¥æ˜¯å¦æœ‰å†å²æ•°æ®æ®‹ç•™ï¼ˆç”¨äºç”¨æˆ·æç¤ºï¼‰
            has_disk_data_before = self._check_vector_store_disk_state()

            # åˆ›å»ºå‘é‡å­˜å‚¨
            success = self._create_vector_store(texts)
            if not success:
                return "âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥"

            # æ›´æ–°æ–‡ä»¶è®°å½•
            self._update_file_record(file_name, len(documents), len(texts))

            self.logger.info("æ–‡æ¡£å¤„ç†å®Œæˆ", extra={
                "file_name": file_name,
                "sections": len(documents),
                "chunks": len(texts)
            })

            # æ„å»ºè¿”å›æ¶ˆæ¯
            result_message = f"âœ… æ–‡æ¡£ '{file_name}' å¤„ç†å®Œæˆï¼\\nğŸ“„ æ–‡æ¡£æ®µè½: {len(documents)}\\nğŸ“ æ–‡æ¡£ç‰‡æ®µ: {len(texts)}"

            # å¦‚æœæ¸…ç†äº†å†å²æ•°æ®ï¼Œæ·»åŠ æç¤ºä¿¡æ¯
            if has_disk_data_before and app_state.vectorstore is not None:
                result_message += "\\nğŸ§¹ å·²è‡ªåŠ¨æ¸…ç†å†å²å‘é‡æ•°æ®ï¼Œç¡®ä¿æ£€ç´¢ç»“æœåŸºäºå½“å‰ä¼šè¯æ–‡æ¡£"

            return result_message

        except Exception as e:
            error_msg = f"âŒ å¤„ç†æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            self.logger.error("æ–‡æ¡£å¤„ç†å¤±è´¥", exception=e, extra={"file": str(file)})
            return error_msg

    # å‘åå…¼å®¹æ–¹æ³•
    def process_pdf(self, file) -> str:
        """
        å‘åå…¼å®¹çš„PDFå¤„ç†æ–¹æ³•
        å†…éƒ¨è°ƒç”¨æ–°çš„process_documentæ–¹æ³•
        """
        return self.process_document(file)

    def process_document_and_update_status(self, file, selected_model: str) -> Tuple[str, str, str, str]:
        """
        å¤„ç†æ–‡æ¡£å¹¶æ›´æ–°ç³»ç»ŸçŠ¶æ€

        Args:
            file: ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡
            selected_model: é€‰æ‹©çš„æ¨¡å‹

        Returns:
            (upload_status, model_status, system_status, file_list)
        """
        try:
            # æ›´æ–°å½“å‰æ¨¡å‹
            app_state.current_model = selected_model

            # å¤„ç†æ–‡æ¡£
            upload_status = self.process_document(file)

            # è·å–æ›´æ–°åçš„çŠ¶æ€
            model_status = f"å½“å‰æ¨¡å‹: {app_state.current_model} (å°±ç»ª)"
            system_status = self._get_system_status()
            file_list = self._get_uploaded_files_display()

            return upload_status, model_status, system_status, file_list

        except Exception as e:
            error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
            return error_msg, "æ¨¡å‹çŠ¶æ€è·å–å¤±è´¥", "ç³»ç»ŸçŠ¶æ€è·å–å¤±è´¥", "æ–‡ä»¶åˆ—è¡¨è·å–å¤±è´¥"

    # å‘åå…¼å®¹æ–¹æ³•
    def process_pdf_and_update_status(self, file, selected_model: str) -> Tuple[str, str, str, str]:
        """
        å‘åå…¼å®¹çš„PDFå¤„ç†å’ŒçŠ¶æ€æ›´æ–°æ–¹æ³•
        å†…éƒ¨è°ƒç”¨æ–°çš„process_document_and_update_statusæ–¹æ³•
        """
        return self.process_document_and_update_status(file, selected_model)

    def _get_file_info(self, file) -> Tuple[str, str]:
        """è·å–æ–‡ä»¶è·¯å¾„å’Œåç§°"""
        if hasattr(file, 'name'):
            file_path = file.name
            file_name = Path(file_path).name
        else:
            file_path = str(file)
            file_name = Path(file_path).name
        return file_path, file_name

    def _validate_file(self, file_path: str) -> bool:
        """éªŒè¯æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ"""
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            self.logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False

        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not self.utility_service.validate_file_type(file_path, self.config.ALLOWED_FILE_TYPES):
            self.logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path}")
            return False

        # éªŒè¯æ–‡ä»¶å¤§å°
        if not self.utility_service.validate_file_size(file_path, self.config.MAX_FILE_SIZE_MB):
            self.logger.error(f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {file_path}")
            return False

        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼ˆç”¨äºå»é‡æ£€æµ‹ï¼‰
        file_hash = self.utility_service.calculate_file_hash(file_path)
        if file_hash:
            self.logger.debug(f"æ–‡ä»¶å“ˆå¸Œè®¡ç®—å®Œæˆ: {file_path} -> {file_hash[:8]}...")

        return True

    def _load_document(self, file_path: str):
        """æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åˆé€‚çš„åŠ è½½å™¨"""
        file_extension = Path(file_path).suffix.lower()

        self.logger.info(f"æ­£åœ¨åŠ è½½æ–‡æ¡£: {file_path}ï¼Œæ–‡ä»¶ç±»å‹: {file_extension}")

        try:
            documents = []

            if file_extension == ".pdf":
                # ä½¿ç”¨PyPDFLoaderå¤„ç†PDF
                loader = PyPDFLoader(file_path)
                documents = loader.load()
                self.logger.info(f"PDFæ–‡æ¡£åŠ è½½æˆåŠŸï¼Œå…± {len(documents)} é¡µ")

            elif file_extension == ".txt":
                # ä½¿ç”¨TextLoaderå¤„ç†æ–‡æœ¬æ–‡ä»¶
                loader = TextLoader(file_path, encoding='utf-8')
                documents = loader.load()
                self.logger.info(f"æ–‡æœ¬æ–‡æ¡£åŠ è½½æˆåŠŸï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")

            elif file_extension == ".md":
                # ä½¿ç”¨UnstructuredMarkdownLoaderå¤„ç†Markdown
                try:
                    loader = UnstructuredMarkdownLoader(file_path)
                    documents = loader.load()
                    self.logger.info(f"Markdownæ–‡æ¡£åŠ è½½æˆåŠŸï¼Œå…± {len(documents)} ä¸ªæ®µè½")
                except Exception as e:
                    self.logger.warning(f"UnstructuredMarkdownLoaderå¤±è´¥: {e}ï¼Œå°è¯•TextLoader")
                    loader = TextLoader(file_path, encoding='utf-8')
                    documents = loader.load()

            elif file_extension == ".docx":
                # Wordæ–‡æ¡£å¤„ç†
                try:
                    loader = UnstructuredWordDocumentLoader(file_path)
                    documents = loader.load()
                    self.logger.info(f"Wordæ–‡æ¡£ï¼ˆUnstructuredï¼‰åŠ è½½æˆåŠŸï¼Œå…± {len(documents)} ä¸ªæ®µè½")

                    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
                    total_content = "".join([doc.page_content.strip() for doc in documents])
                    if not total_content:
                        self.logger.warning("Wordæ–‡æ¡£æ–‡å­—å†…å®¹ä¸ºç©ºï¼Œå°è¯•æå–å›¾ç‰‡ä¸­çš„æ–‡å­—")

                        # æå–å¹¶è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
                        images = self._extract_images_from_docx(file_path)
                        if images:
                            self.logger.info(f"æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹æ–‡å­—è¯†åˆ«...")

                            all_text_content = []
                            for i, image in enumerate(images):
                                self.logger.info(f"æ­£åœ¨è¯†åˆ«ç¬¬ {i+1} å¼ å›¾ç‰‡...")
                                text_content = self._recognize_text_from_image(image)
                                if text_content.strip():
                                    all_text_content.append(f"å›¾ç‰‡ {i+1} å†…å®¹:\n{text_content}")

                            if all_text_content:
                                combined_content = "\n\n".join(all_text_content)
                                documents = [Document(
                                    page_content=combined_content,
                                    metadata={"source": file_path, "extraction_method": "gemini_vision"}
                                )]
                                self.logger.info(f"å›¾ç‰‡æ–‡å­—è¯†åˆ«å®Œæˆï¼Œæ€»å†…å®¹é•¿åº¦: {len(combined_content)} å­—ç¬¦")
                            else:
                                self.logger.warning("å›¾ç‰‡ä¸­æœªè¯†åˆ«åˆ°æœ‰æ•ˆæ–‡å­—")
                        else:
                            self.logger.warning("Wordæ–‡æ¡£ä¸­æœªæ‰¾åˆ°å›¾ç‰‡")

                except Exception as e:
                    self.logger.warning(f"UnstructuredWordDocumentLoaderå¤±è´¥: {e}ï¼Œå°è¯•python-docxç›´æ¥è¯»å–")
                    try:
                        doc = DocxDocument(file_path)
                        content = "\n".join([paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()])

                        if content.strip():
                            documents = [Document(page_content=content, metadata={"source": file_path})]
                            self.logger.info(f"Wordæ–‡æ¡£ï¼ˆpython-docxï¼‰åŠ è½½æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                        else:
                            # å¦‚æœæ²¡æœ‰æ–‡å­—ï¼Œå°è¯•æå–å›¾ç‰‡
                            self.logger.info("æ–‡æ¡£æ— æ–‡å­—å†…å®¹ï¼Œå°è¯•æå–å›¾ç‰‡...")
                            images = self._extract_images_from_docx(file_path)
                            if images:
                                all_text_content = []
                                for i, image in enumerate(images):
                                    text_content = self._recognize_text_from_image(image)
                                    if text_content.strip():
                                        all_text_content.append(f"å›¾ç‰‡ {i+1} å†…å®¹:\n{text_content}")

                                if all_text_content:
                                    combined_content = "\n\n".join(all_text_content)
                                    documents = [Document(
                                        page_content=combined_content,
                                        metadata={"source": file_path, "extraction_method": "gemini_vision"}
                                    )]
                                    self.logger.info(f"å›¾ç‰‡æ–‡å­—è¯†åˆ«å®Œæˆï¼Œæ€»å†…å®¹é•¿åº¦: {len(combined_content)} å­—ç¬¦")

                    except Exception as e2:
                        self.logger.error(f"python-docxä¹Ÿå¤±è´¥äº†: {e2}")

            elif file_extension == ".xlsx":
                # Excelæ–‡æ¡£å¤„ç†
                try:
                    loader = UnstructuredExcelLoader(file_path)
                    documents = loader.load()
                    self.logger.info(f"Excelæ–‡æ¡£ï¼ˆUnstructuredï¼‰åŠ è½½æˆåŠŸï¼Œå…± {len(documents)} ä¸ªè¡¨æ ¼")
                except Exception as e:
                    self.logger.warning(f"UnstructuredExcelLoaderå¤±è´¥: {e}ï¼Œå°è¯•pandasç›´æ¥è¯»å–")
                    try:
                        # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
                        excel_file = pd.ExcelFile(file_path)
                        all_content = []

                        for sheet_name in excel_file.sheet_names:
                            df = pd.read_excel(file_path, sheet_name=sheet_name)
                            sheet_content = f"å·¥ä½œè¡¨: {sheet_name}\n{df.to_string(index=False)}"
                            all_content.append(sheet_content)

                        if all_content:
                            combined_content = "\n\n".join(all_content)
                            documents = [Document(page_content=combined_content, metadata={"source": file_path})]
                            self.logger.info(f"Excelæ–‡æ¡£ï¼ˆpandasï¼‰åŠ è½½æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(combined_content)} å­—ç¬¦")

                    except Exception as e2:
                        self.logger.error(f"pandasè¯»å–Excelä¹Ÿå¤±è´¥äº†: {e2}")

            elif file_extension == ".pptx":
                # PowerPointæ–‡æ¡£å¤„ç†
                try:
                    loader = UnstructuredPowerPointLoader(file_path)
                    documents = loader.load()
                    self.logger.info(f"PowerPointæ–‡æ¡£ï¼ˆUnstructuredï¼‰åŠ è½½æˆåŠŸï¼Œå…± {len(documents)} ä¸ªå¹»ç¯ç‰‡")
                except Exception as e:
                    self.logger.warning(f"UnstructuredPowerPointLoaderå¤±è´¥: {e}ï¼Œå°è¯•python-pptxç›´æ¥è¯»å–")
                    try:
                        prs = Presentation(file_path)
                        all_content = []

                        for i, slide in enumerate(prs.slides):
                            slide_content = f"å¹»ç¯ç‰‡ {i+1}:\n"
                            for shape in slide.shapes:
                                if hasattr(shape, "text") and shape.text.strip():
                                    slide_content += f"{shape.text}\n"
                            all_content.append(slide_content)

                        if all_content:
                            combined_content = "\n\n".join(all_content)
                            documents = [Document(page_content=combined_content, metadata={"source": file_path})]
                            self.logger.info(f"PowerPointæ–‡æ¡£ï¼ˆpython-pptxï¼‰åŠ è½½æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(combined_content)} å­—ç¬¦")

                    except Exception as e2:
                        self.logger.error(f"python-pptxè¯»å–PowerPointä¹Ÿå¤±è´¥äº†: {e2}")

            else:
                self.logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}")
                return []

            # éªŒè¯æ–‡æ¡£å†…å®¹
            for i, doc in enumerate(documents):
                content_length = len(doc.page_content.strip())
                self.logger.info(f"æ–‡æ¡£ {i+1} å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦ï¼Œå†…å®¹é¢„è§ˆ: {doc.page_content[:100]}...")

                if content_length == 0:
                    self.logger.warning(f"æ–‡æ¡£ {i+1} å†…å®¹ä¸ºç©º")

            return documents

        except Exception as e:
            self.logger.error(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
            return []

    def _split_documents(self, documents):
        """æ™ºèƒ½åˆ†å‰²æ–‡æ¡£ï¼ˆæ”¯æŒè¯­ä¹‰åˆ†å—å’Œä¼ ç»Ÿåˆ†å—ï¼‰"""
        self.logger.info("æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")

        # æ£€æŸ¥é…ç½®å’Œåˆ†å—å™¨å¯ç”¨æ€§
        use_semantic = self.config.USE_SEMANTIC_CHUNKING
        fallback_enabled = self.config.FALLBACK_TO_TRADITIONAL

        texts = []
        split_method = "unknown"

        try:
            if use_semantic and self.semantic_splitter:
                # å°è¯•è¯­ä¹‰åˆ†å—
                self.logger.info("å°è¯•ä½¿ç”¨è¯­ä¹‰åˆ†å—å™¨...")
                texts = self.semantic_splitter.split_documents(documents, use_semantic=True)
                split_method = "semantic"

                # éªŒè¯è¯­ä¹‰åˆ†å—ç»“æœ
                if not texts and fallback_enabled:
                    self.logger.warning("è¯­ä¹‰åˆ†å—ç»“æœä¸ºç©ºï¼Œå›é€€åˆ°ä¼ ç»Ÿåˆ†å—")
                    texts = self.traditional_splitter.split_documents(documents)
                    split_method = "traditional_fallback"

            else:
                # ç›´æ¥ä½¿ç”¨ä¼ ç»Ÿåˆ†å—å™¨
                self.logger.info("ä½¿ç”¨ä¼ ç»Ÿåˆ†å—å™¨...")
                texts = self.traditional_splitter.split_documents(documents)
                split_method = "traditional"

        except Exception as e:
            self.logger.error(f"åˆ†å—è¿‡ç¨‹å‡ºé”™: {str(e)}")

            if fallback_enabled and split_method != "traditional":
                self.logger.info("é”™è¯¯å›é€€åˆ°ä¼ ç»Ÿåˆ†å—...")
                try:
                    texts = self.traditional_splitter.split_documents(documents)
                    split_method = "traditional_error_fallback"
                except Exception as fallback_error:
                    self.logger.error(f"ä¼ ç»Ÿåˆ†å—ä¹Ÿå¤±è´¥: {str(fallback_error)}")
                    texts = []

        # å¦‚æœåˆ†å‰²åæ²¡æœ‰æ–‡æ¡£ç‰‡æ®µï¼Œæ£€æŸ¥åŸæ–‡æ¡£å†…å®¹
        if not texts and documents:
            self.logger.warning("æ–‡æ¡£åˆ†å‰²åä¸ºç©ºï¼Œæ£€æŸ¥åŸæ–‡æ¡£å†…å®¹é•¿åº¦")
            for i, doc in enumerate(documents):
                content_length = len(doc.page_content.strip())
                self.logger.info(f"åŸæ–‡æ¡£ {i+1} å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")

                # å¦‚æœåŸæ–‡æ¡£å†…å®¹å¤ªçŸ­ï¼Œç›´æ¥ä½¿ç”¨åŸæ–‡æ¡£
                if content_length > 0:
                    self.logger.info("ä½¿ç”¨åŸæ–‡æ¡£ä½œä¸ºæ–‡æ¡£ç‰‡æ®µï¼ˆå†…å®¹è¾ƒçŸ­ï¼‰")
                    texts = documents
                    split_method = "no_split"
                    break

        # è®°å½•åˆ†å—ç»Ÿè®¡ä¿¡æ¯
        chunk_stats = self._analyze_chunk_stats(texts)

        self.logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆ", extra={
            "chunks": len(texts),
            "split_method": split_method,
            "avg_chunk_size": chunk_stats.get("avg_size", 0),
            "min_chunk_size": chunk_stats.get("min_size", 0),
            "max_chunk_size": chunk_stats.get("max_size", 0)
        })

        return texts

    def _analyze_chunk_stats(self, chunks) -> dict:
        """åˆ†æåˆ†å—ç»Ÿè®¡ä¿¡æ¯"""
        if not chunks:
            return {"avg_size": 0, "min_size": 0, "max_size": 0}

        sizes = [len(chunk.page_content) for chunk in chunks]

        return {
            "avg_size": sum(sizes) // len(sizes) if sizes else 0,
            "min_size": min(sizes) if sizes else 0,
            "max_size": max(sizes) if sizes else 0,
            "total_chunks": len(chunks)
        }

    def _create_vector_store(self, texts) -> bool:
        """åˆ›å»ºæˆ–æ›´æ–°å‘é‡å­˜å‚¨ï¼ˆå¢é‡æ¨¡å¼ï¼‰"""
        try:
            self.logger.info("æ­£åœ¨å¤„ç†å‘é‡æ•°æ®åº“...")

            # éªŒè¯æ–‡æ¡£ç‰‡æ®µ
            if not texts:
                self.logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£ç‰‡æ®µç”¨äºåˆ›å»ºå‘é‡æ•°æ®åº“")
                return False

            # è¿‡æ»¤ç©ºå†…å®¹çš„æ–‡æ¡£
            valid_texts = [doc for doc in texts if doc.page_content.strip()]

            if not valid_texts:
                self.logger.error("æ‰€æœ‰æ–‡æ¡£ç‰‡æ®µéƒ½æ˜¯ç©ºçš„")
                return False

            self.logger.info(f"æœ‰æ•ˆæ–‡æ¡£ç‰‡æ®µæ•°é‡: {len(valid_texts)}")

            # ä½¿ç”¨é…ç½®ä¸­çš„ChromaDBè·¯å¾„
            persist_directory = self.config.CHROMA_DB_PATH

                        # æ£€æŸ¥åº”ç”¨çŠ¶æ€å’Œç£ç›˜çŠ¶æ€
            existing_vectorstore = app_state.vectorstore
            has_disk_data = self._check_vector_store_disk_state()

            if existing_vectorstore is None:
                # åº”ç”¨çŠ¶æ€ä¸­æ²¡æœ‰å‘é‡å­˜å‚¨
                if has_disk_data:
                    # æ£€æµ‹åˆ°ç£ç›˜ä¸Šæœ‰å†å²æ•°æ®ï¼Œå…ˆæ¸…ç†ä»¥ç¡®ä¿å¹²å‡€çŠ¶æ€
                    self.logger.warning("æ£€æµ‹åˆ°å†å²å‘é‡æ•°æ®æ®‹ç•™ï¼Œæ­£åœ¨æ¸…ç†ä»¥ç¡®ä¿å¹²å‡€çŠ¶æ€...")
                    if not self._clear_vector_store():
                        self.logger.error("æ¸…ç†å†å²æ•°æ®å¤±è´¥")
                        return False

                # åˆ›å»ºå…¨æ–°çš„å‘é‡æ•°æ®åº“
                self.logger.info("åˆ›å»ºå…¨æ–°å‘é‡æ•°æ®åº“...")
                vectorstore = Chroma.from_documents(
                    documents=valid_texts,
                    embedding=self.embeddings,
                    persist_directory=persist_directory
                )

                # è®¾ç½®åˆ°åº”ç”¨çŠ¶æ€ä¸­
                app_state.vectorstore = vectorstore
                # é‡ç½®QAé“¾ä»¥ä¾¿ä½¿ç”¨æ–°çš„å‘é‡å­˜å‚¨
                app_state.qa_chain = None

                self.logger.info("å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ", extra={
                    "documents_added": len(valid_texts),
                    "operation": "create_clean",
                    "disk_cleaned": has_disk_data
                })
            else:
                # å¢é‡æ·»åŠ åˆ°ç°æœ‰å‘é‡å­˜å‚¨
                self.logger.info("å‘å·²æœ‰å‘é‡æ•°æ®åº“å¢é‡æ·»åŠ æ–‡æ¡£...")
                try:
                    # ä½¿ç”¨ add_documents æ–¹æ³•å¢é‡æ·»åŠ 
                    existing_vectorstore.add_documents(valid_texts)

                    # ç¡®ä¿å‘é‡å­˜å‚¨ä»åœ¨åº”ç”¨çŠ¶æ€ä¸­
                    app_state.vectorstore = existing_vectorstore
                    # é‡ç½®QAé“¾ä»¥ä¾¿åˆ·æ–°æ£€ç´¢å™¨
                    app_state.qa_chain = None

                    self.logger.info("æ–‡æ¡£æˆåŠŸå¢é‡æ·»åŠ åˆ°å‘é‡æ•°æ®åº“", extra={
                        "documents_added": len(valid_texts),
                        "operation": "add"
                    })
                except Exception as add_error:
                    self.logger.error(f"å¢é‡æ·»åŠ å¤±è´¥: {str(add_error)}ï¼Œå°è¯•é‡æ–°åˆ›å»ºå‘é‡å­˜å‚¨")
                    # å¦‚æœå¢é‡æ·»åŠ å¤±è´¥ï¼Œå°è¯•é‡æ–°åˆ›å»ºæ•´ä¸ªå‘é‡å­˜å‚¨
                    vectorstore = Chroma.from_documents(
                        documents=valid_texts,
                        embedding=self.embeddings,
                        persist_directory=persist_directory
                    )

                    app_state.vectorstore = vectorstore
                    app_state.qa_chain = None

                    self.logger.warning("å‘é‡æ•°æ®åº“å·²é‡æ–°åˆ›å»ºï¼ˆå¢é‡æ·»åŠ å¤±è´¥åçš„å›é€€æ“ä½œï¼‰", extra={
                        "documents_added": len(valid_texts),
                        "operation": "recreate_fallback"
                    })

            return True

        except Exception as e:
            self.logger.error(f"å‘é‡æ•°æ®åº“å¤„ç†å¤±è´¥: {str(e)}")
            return False

    def _clear_vector_store(self) -> bool:
        """æ¸…ç†å‘é‡å­˜å‚¨ï¼ˆåˆ é™¤æŒä¹…åŒ–æ•°æ®ï¼‰"""
        try:
            persist_directory = self.config.CHROMA_DB_PATH
            persist_path = Path(persist_directory)

            if persist_path.exists():
                self.logger.info(f"æ£€æµ‹åˆ°ç°æœ‰å‘é‡å­˜å‚¨ç›®å½•: {persist_directory}")

                # å®‰å…¨åˆ é™¤ç›®å½•åŠæ‰€æœ‰å†…å®¹
                shutil.rmtree(persist_path)
                self.logger.info("å‘é‡å­˜å‚¨ç›®å½•å·²æ¸…ç†", extra={
                    "operation": "clear",
                    "directory": str(persist_path)
                })
            else:
                self.logger.info("å‘é‡å­˜å‚¨ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")

            # é‡ç½®åº”ç”¨çŠ¶æ€ä¸­çš„å‘é‡å­˜å‚¨
            app_state.vectorstore = None
            app_state.qa_chain = None

            self.logger.info("å‘é‡å­˜å‚¨çŠ¶æ€å·²é‡ç½®")
            return True

        except Exception as e:
            self.logger.error(f"æ¸…ç†å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
            return False

    def _check_vector_store_disk_state(self) -> bool:
        """æ£€æŸ¥ç£ç›˜ä¸Šæ˜¯å¦å­˜åœ¨å‘é‡å­˜å‚¨æ•°æ®"""
        try:
            persist_directory = self.config.CHROMA_DB_PATH
            persist_path = Path(persist_directory)

            if not persist_path.exists():
                return False

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ChromaDBç›¸å…³æ–‡ä»¶
            has_chroma_db = any([
                (persist_path / "chroma.sqlite3").exists(),
                any(persist_path.glob("*.sqlite3")),
                any(persist_path.iterdir())  # ä»»ä½•æ–‡ä»¶æˆ–ç›®å½•
            ])

            if has_chroma_db:
                self.logger.info("æ£€æµ‹åˆ°ç£ç›˜ä¸Šå­˜åœ¨å‘é‡å­˜å‚¨æ•°æ®", extra={
                    "directory": str(persist_path),
                    "files": [f.name for f in persist_path.iterdir()]
                })

            return has_chroma_db

        except Exception as e:
            self.logger.error(f"æ£€æŸ¥å‘é‡å­˜å‚¨ç£ç›˜çŠ¶æ€å¤±è´¥: {str(e)}")
            return False

    def _extract_images_from_docx(self, docx_path: str) -> List[Image.Image]:
        """ä»Wordæ–‡æ¡£ä¸­æå–å›¾ç‰‡"""
        images = []
        try:
            doc = DocxDocument(docx_path)

            # éå†æ–‡æ¡£ä¸­çš„æ‰€æœ‰å…³ç³»ï¼ˆåŒ…æ‹¬å›¾ç‰‡ï¼‰
            for rel in doc.part.rels.values():
                if "image" in rel.target_ref:
                    try:
                        # è¯»å–å›¾ç‰‡æ•°æ®
                        image_data = rel.target_part.blob
                        # è½¬æ¢ä¸ºPIL Image
                        image = Image.open(io.BytesIO(image_data))
                        images.append(image)
                        self.logger.info(f"æå–åˆ°å›¾ç‰‡ï¼Œå°ºå¯¸: {image.size}")
                    except Exception as e:
                        self.logger.warning(f"æå–å›¾ç‰‡æ—¶å‡ºé”™: {e}")

        except Exception as e:
            self.logger.error(f"ä»Wordæ–‡æ¡£æå–å›¾ç‰‡å¤±è´¥: {e}")

        return images

    def _recognize_text_from_image(self, image: Image.Image) -> str:
        """ä½¿ç”¨Google Gemini Vision APIè¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—"""
        try:
            # å°†PILå›¾ç‰‡è½¬æ¢ä¸ºbase64æ ¼å¼
            buffer = io.BytesIO()
            # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆå»é™¤alphaé€šé“ï¼‰
            if image.mode in ('RGBA', 'LA', 'P'):
                image = image.convert('RGB')
            image.save(buffer, format='JPEG', quality=95)
            image_data = buffer.getvalue()

            # è°ƒç”¨Gemini Vision API
            prompt = """
            è¯·æå–è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ã€‚
            è¦æ±‚ï¼š
            1. ä¿æŒåŸæœ‰çš„æ ¼å¼å’Œç»“æ„
            2. å¦‚æœæœ‰è¡¨æ ¼ï¼Œè¯·ç”¨åˆé€‚çš„æ ¼å¼è¡¨ç¤º
            3. å¦‚æœæœ‰åˆ—è¡¨ï¼Œè¯·ä¿æŒåˆ—è¡¨æ ¼å¼
            4. åªè¿”å›æ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ è¯´æ˜
            """

            response = self.vision_model.generate_content([
                prompt,
                {
                    'mime_type': 'image/jpeg',
                    'data': image_data
                }
            ])

            text_content = response.text if response.text else ""
            self.logger.info(f"Gemini Vision è¯†åˆ«åˆ°æ–‡å­—é•¿åº¦: {len(text_content)} å­—ç¬¦")

            return text_content

        except Exception as e:
            self.logger.error(f"Gemini Vision APIè°ƒç”¨å¤±è´¥: {e}")
            return ""

    def _update_file_record(self, file_name: str, pages: int, chunks: int):
        """æ›´æ–°æ–‡ä»¶è®°å½•"""
        file_info = FileInfo(
            name=file_name,
            upload_time=datetime.now(),
            pages=pages,
            chunks=chunks,
            model=app_state.current_model
        )
        app_state.add_uploaded_file(file_info)

    def _get_system_status(self) -> str:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            state_info = app_state.get_state_info()

            status_parts = [
                f"ğŸ¤– å½“å‰æ¨¡å‹: {state_info['current_model']}",
                f"ğŸ“Š çŠ¶æ€: {'å°±ç»ª' if state_info['qa_chain_initialized'] else 'æœªåŠ è½½'}",
                f"ğŸ“š å·²ä¸Šä¼ æ–‡æ¡£: {state_info['uploaded_files_count']} ä¸ª",
                f"ğŸ”§ å‘é‡æ•°æ®åº“: {'å·²åˆå§‹åŒ–' if state_info['vectorstore_initialized'] else 'æœªåˆå§‹åŒ–'}",
                f"ğŸ• æœ€åæ›´æ–°: {datetime.fromisoformat(state_info['last_update']).strftime('%H:%M:%S')}"
            ]

            return "\\n".join(status_parts)

        except Exception as e:
            return f"âŒ è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}"

    def _get_uploaded_files_display(self) -> str:
        """è·å–ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨æ˜¾ç¤º"""
        try:
            files = app_state.get_uploaded_files()

            if not files:
                return "æš‚æ— å·²ä¸Šä¼ çš„æ–‡ä»¶"

            file_list = ["## ğŸ“ å·²ä¸Šä¼ çš„æ–‡ä»¶\\n"]

            for i, file_info in enumerate(files, 1):
                upload_time = file_info.upload_time.strftime("%Y-%m-%d %H:%M:%S")
                file_list.append(
                    f"**{i}. {file_info.name}**\\n"
                    f"   - ğŸ“… ä¸Šä¼ æ—¶é—´: {upload_time}\\n"
                    f"   - ğŸ“„ é¡µæ•°: {file_info.pages}\\n"
                    f"   - ğŸ“ æ–‡æ¡£ç‰‡æ®µ: {file_info.chunks}\\n"
                    f"   - ğŸ¤– å¤„ç†æ¨¡å‹: {file_info.model}\\n"
                )

            return "\\n".join(file_list)

        except Exception as e:
            return f"âŒ è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"

    def get_uploaded_files_count(self) -> int:
        """è·å–å·²ä¸Šä¼ æ–‡ä»¶æ•°é‡"""
        return len(app_state.get_uploaded_files())

    def clear_uploaded_files(self):
        """æ¸…ç©ºä¸Šä¼ æ–‡ä»¶è®°å½•"""
        app_state.clear_uploaded_files()

    def clear_all_documents_and_storage(self) -> str:
        """æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£è®°å½•å’Œå‘é‡å­˜å‚¨"""
        try:
            # æ¸…ç©ºä¸Šä¼ æ–‡ä»¶è®°å½•
            self.clear_uploaded_files()

            # æ¸…ç†å‘é‡å­˜å‚¨
            if self._clear_vector_store():
                self.logger.info("æ‰€æœ‰æ–‡æ¡£å’Œå‘é‡å­˜å‚¨å·²æ¸…ç†")
                return "âœ… å·²æ¸…ç©ºæ‰€æœ‰æ–‡æ¡£è®°å½•å’Œå‘é‡å­˜å‚¨"
            else:
                return "âŒ æ¸…ç†å‘é‡å­˜å‚¨æ—¶å‘ç”Ÿé”™è¯¯"

        except Exception as e:
            error_msg = f"âŒ æ¸…ç†æ“ä½œå¤±è´¥: {str(e)}"
            self.logger.error("æ¸…ç†æ‰€æœ‰æ•°æ®å¤±è´¥", exception=e)
            return error_msg