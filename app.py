#!/usr/bin/env python3
"""
Web RAG ç³»ç»Ÿ - ä¸»ç¨‹åº
åŸºäº Google Gemini çš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
"""

import gradio as gr
import os
import sys
import tempfile
import traceback
from pathlib import Path

# å¯¼å…¥é…ç½®ç®¡ç†
from config import Config

# éªŒè¯é…ç½®
if not Config.validate_config():
    print("âŒ é…ç½®é”™è¯¯ï¼šè¯·è®¾ç½®GOOGLE_API_KEYç¯å¢ƒå˜é‡")
    print("ğŸ’¡ æç¤ºï¼š")
    print("   1. å¤åˆ¶ .env.example ä¸º .env")
    print("   2. åœ¨ .env æ–‡ä»¶ä¸­å¡«å…¥æ‚¨çš„ Google API Key")
    print("   3. æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: export GOOGLE_API_KEY=your_key_here")
    print("   4. è·å–API Key: https://aistudio.google.com/")
    sys.exit(1)

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä»é…ç½®ä¸­è¯»å–ï¼‰
os.environ["GOOGLE_API_KEY"] = Config.GOOGLE_API_KEY

try:
    from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
    from langchain_community.document_loaders import PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate

    # å…¨å±€å˜é‡
    vectorstore = None
    qa_chain = None

    def create_llm():
        """åˆ›å»º LLMï¼Œå°è¯•å¤šä¸ªæ¨¡å‹åç§°"""
        model_names = [
            # æœ€æ–°çš„ 2.5 ç³»åˆ—æ¨¡å‹ï¼ˆPreviewï¼‰
            "gemini-2.5-flash-preview-05-20",
            "gemini-2.5-pro-preview-06-05",

            # 2.0 ç³»åˆ—æ¨¡å‹ï¼ˆç¨³å®šç‰ˆï¼‰
            "gemini-2.0-flash",
            "gemini-2.0-flash-lite",

            # 1.5 ç³»åˆ—æ¨¡å‹ï¼ˆç¨³å®šç‰ˆï¼Œå¤‡ç”¨ï¼‰
            "gemini-1.5-flash",
            "gemini-1.5-pro",
        ]

        for model_name in model_names:
            try:
                print(f"å°è¯•æ¨¡å‹: {model_name}")
                llm = ChatGoogleGenerativeAI(
                    model=model_name,
                    temperature=0.3,
                    convert_system_message_to_human=True
                )
                # æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
                test_response = llm.invoke("æµ‹è¯•")
                print(f"âœ… æˆåŠŸä½¿ç”¨æ¨¡å‹: {model_name}")
                return llm
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                continue

        raise Exception("æ‰€æœ‰ Gemini æ¨¡å‹éƒ½ä¸å¯ç”¨")

    def process_pdf(file):
        """å¤„ç† PDF æ–‡ä»¶å¹¶åˆ›å»ºå‘é‡æ•°æ®åº“"""
        global vectorstore, qa_chain

        print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file}")

        if file is None:
            return "âŒ è¯·é€‰æ‹©ä¸€ä¸ª PDF æ–‡ä»¶"

        try:
            # è·å–æ–‡ä»¶è·¯å¾„
            if hasattr(file, 'name'):
                file_path = file.name
                file_name = Path(file_path).name
            else:
                file_path = str(file)
                file_name = Path(file_path).name

            print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
            print(f"æ–‡ä»¶å: {file_name}")

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"

            # åŠ è½½ PDF
            print("æ­£åœ¨åŠ è½½ PDF...")
            loader = PyPDFLoader(file_path)
            documents = loader.load()

            if not documents:
                return "âŒ PDF æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–"

            print(f"æˆåŠŸåŠ è½½ {len(documents)} é¡µæ–‡æ¡£")

            # åˆ†å‰²æ–‡æ¡£
            print("æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
            )
            texts = text_splitter.split_documents(documents)
            print(f"æ–‡æ¡£åˆ†å‰²ä¸º {len(texts)} ä¸ªç‰‡æ®µ")

            # åˆ›å»ºåµŒå…¥
            print("æ­£åœ¨åˆ›å»ºåµŒå…¥...")
            embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

            # åˆ›å»ºå‘é‡æ•°æ®åº“
            print("æ­£åœ¨åˆ›å»ºå‘é‡æ•°æ®åº“...")
            vectorstore = Chroma.from_documents(
                documents=texts,
                embedding=embeddings,
                persist_directory="./chroma_db"
            )
            print("å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ")

            # åˆ›å»º QA é“¾
            print("æ­£åœ¨åˆå§‹åŒ– QA é“¾...")
            llm = create_llm()  # ä½¿ç”¨æ–°çš„ LLM åˆ›å»ºå‡½æ•°

            # è‡ªå®šä¹‰æç¤ºæ¨¡æ¿
            prompt_template = """
            åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚

            ä¸Šä¸‹æ–‡ï¼š
            {context}

            é—®é¢˜ï¼š{question}

            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š
            """

            PROMPT = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )

            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True
            )

            print("QA é“¾åˆå§‹åŒ–æˆåŠŸ")

            result_message = f"""âœ… æˆåŠŸå¤„ç† PDF æ–‡ä»¶: {file_name}
ğŸ“„ å…±å¤„ç† {len(documents)} é¡µæ–‡æ¡£
ğŸ” åˆ†å‰²ä¸º {len(texts)} ä¸ªæ–‡æ¡£ç‰‡æ®µ
ğŸ’¾ å‘é‡æ•°æ®åº“å·²åˆ›å»º
ğŸ¤– QA é“¾å·²åˆå§‹åŒ–
ğŸ’¡ ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†ï¼"""

            print("å¤„ç†å®Œæˆ")
            return result_message

        except Exception as e:
            error_msg = f"âŒ å¤„ç† PDF æ—¶å‡ºé”™: {str(e)}"
            print(f"é”™è¯¯: {e}")
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return error_msg

    def chat_with_pdf(message, history):
        """ä¸ PDF å†…å®¹å¯¹è¯"""
        global qa_chain

        print(f"æ”¶åˆ°æ¶ˆæ¯: {message}")

        if not message.strip():
            return history, ""

        if qa_chain is None:
            response = "âŒ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç† PDF æ–‡ä»¶"
            history.append([message, response])
            return history, ""

        try:
            print("æ­£åœ¨æŸ¥è¯¢ QA é“¾...")
            # æŸ¥è¯¢ QA é“¾
            result = qa_chain({"query": message})
            response = result["result"]

            print(f"è·å¾—å›ç­”: {response[:100]}...")

            # æ·»åŠ æºæ–‡æ¡£ä¿¡æ¯
            if "source_documents" in result and result["source_documents"]:
                sources = set()
                for doc in result["source_documents"]:
                    if hasattr(doc, 'metadata') and 'source' in doc.metadata:
                        sources.add(Path(doc.metadata['source']).name)

                if sources:
                    response += f"\n\nğŸ“š å‚è€ƒæ¥æº: {', '.join(sources)}"

            history.append([message, response])
            return history, ""

        except Exception as e:
            error_response = f"âŒ æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
            print(f"æŸ¥è¯¢é”™è¯¯: {e}")
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            history.append([message, error_response])
            return history, ""

    def get_system_status():
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        global vectorstore, qa_chain

        status = f"""
## ğŸ“Š ç³»ç»ŸçŠ¶æ€

**Python ç‰ˆæœ¬**: {sys.version.split()[0]}

**å·¥ä½œç›®å½•**: {os.getcwd()}

**API å¯†é’¥**: {'âœ… å·²é…ç½®' if os.getenv('GOOGLE_API_KEY') else 'âŒ æœªé…ç½®'}

**å‘é‡æ•°æ®åº“**: {'âœ… å·²åŠ è½½' if vectorstore else 'âŒ æœªåŠ è½½'}

**QA é“¾**: {'âœ… å·²åˆå§‹åŒ–' if qa_chain else 'âŒ æœªåˆå§‹åŒ–'}

---

## ğŸ“‹ ä½¿ç”¨è¯´æ˜

1. åœ¨"æ–‡æ¡£ä¸Šä¼ "æ ‡ç­¾é¡µä¸Šä¼  PDF æ–‡ä»¶
2. ç­‰å¾…å¤„ç†å®Œæˆï¼ˆæŸ¥çœ‹çŠ¶æ€ä¿¡æ¯ï¼‰
3. åœ¨"æ™ºèƒ½å¯¹è¯"æ ‡ç­¾é¡µæé—®
4. ç³»ç»Ÿä¼šåŸºäºæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜

---

## ğŸ”§ æŠ€æœ¯æ ˆ

**LLM**: Google Gemini (è‡ªåŠ¨é€‰æ‹©å¯ç”¨æ¨¡å‹)

**åµŒå…¥æ¨¡å‹**: Google Embedding-001

**å‘é‡æ•°æ®åº“**: ChromaDB

**æ¡†æ¶**: LangChain + Gradio

---

## ğŸ› è°ƒè¯•ä¿¡æ¯

**å‘é‡å­˜å‚¨å¯¹è±¡**: {type(vectorstore).__name__ if vectorstore else 'None'}

**QA é“¾å¯¹è±¡**: {type(qa_chain).__name__ if qa_chain else 'None'}

---

## ğŸš€ æ”¯æŒçš„ Gemini æ¨¡å‹

**æœ€æ–° 2.5 ç³»åˆ— (Preview)**
- `gemini-2.5-flash-preview-05-20` - æœ€æ–° Flashï¼Œæ”¯æŒæ€ç»´é“¾æ¨ç†
- `gemini-2.5-pro-preview-06-05` - æœ€å¼ºæ¨ç†èƒ½åŠ›ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡

**ç¨³å®š 2.0 ç³»åˆ—**
- `gemini-2.0-flash` - ä¸‹ä¸€ä»£ç‰¹æ€§ï¼Œç”Ÿäº§ç¯å¢ƒæ¨è
- `gemini-2.0-flash-lite` - æˆæœ¬ä¼˜åŒ–ç‰ˆï¼Œé«˜é¢‘è°ƒç”¨

**å¤‡ç”¨ 1.5 ç³»åˆ—**
- `gemini-1.5-flash` - å¿«é€Ÿå¤šæ¨¡æ€å¤„ç†
- `gemini-1.5-pro` - å¤æ‚æ¨ç†ä»»åŠ¡

---

## ğŸ’¡ æ¨¡å‹é€‰æ‹©ç­–ç•¥

ç³»ç»Ÿä¼šè‡ªåŠ¨æŒ‰ä¼˜å…ˆçº§å°è¯•æ¨¡å‹ï¼š
1. **ä¼˜å…ˆ**: æœ€æ–° 2.5 ç³»åˆ—ï¼ˆæ€§èƒ½æœ€ä½³ï¼‰
2. **å¤‡é€‰**: ç¨³å®š 2.0 ç³»åˆ—ï¼ˆç”Ÿäº§å¯é ï¼‰
3. **å…œåº•**: 1.5 ç³»åˆ—ï¼ˆç¡®ä¿å¯ç”¨æ€§ï¼‰

---

## ğŸ¯ æ€§èƒ½ç‰¹ç‚¹

**2.5 Flash Preview**: æ€ç»´é“¾æ¨ç† + å¹³è¡¡æ€§èƒ½

**2.5 Pro Preview**: æœ€å¼ºæ¨ç† + å¤æ‚ä»»åŠ¡

**2.0 Flash**: ä¸‹ä¸€ä»£ç‰¹æ€§ + ç¨³å®šå¯é 

**2.0 Flash-Lite**: æˆæœ¬ä¼˜åŒ– + ä½å»¶è¿Ÿ

**1.5 Flash**: å¿«é€Ÿå“åº” + å¤šæ¨¡æ€

**1.5 Pro**: æ·±åº¦æ¨ç† + é•¿ä¸Šä¸‹æ–‡
"""
        return status

    # åˆ›å»º Gradio ç•Œé¢
    with gr.Blocks(title="Web RAG ç³»ç»Ÿ") as demo:
        gr.Markdown("# ğŸš€ Web RAG ç³»ç»Ÿ")
        gr.Markdown("åŸºäº Google Gemini çš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")

        with gr.Tab("ğŸ“„ æ–‡æ¡£ä¸Šä¼ "):
            gr.Markdown("### ä¸Šä¼  PDF æ–‡æ¡£")
            gr.Markdown("**æ³¨æ„**: ä¸Šä¼ åè¯·ç­‰å¾…å¤„ç†å®Œæˆï¼ŒçŠ¶æ€ä¼šæ˜¾ç¤ºåœ¨ä¸‹æ–¹")

            file_input = gr.File()
            upload_output = gr.Textbox(
                label="å¤„ç†çŠ¶æ€"
            )

            # ç»‘å®šä¸Šä¼ äº‹ä»¶
            file_input.upload(
                fn=process_pdf,
                inputs=file_input,
                outputs=upload_output
            )

        with gr.Tab("ğŸ’¬ æ™ºèƒ½å¯¹è¯"):
            gr.Markdown("### ä¸æ–‡æ¡£å†…å®¹å¯¹è¯")
            gr.Markdown("**æç¤º**: è¯·å…ˆä¸Šä¼ å¹¶å¤„ç† PDF æ–‡ä»¶ï¼Œç„¶ååœ¨æ­¤æé—®")

            chatbot = gr.Chatbot()
            msg = gr.Textbox()
            with gr.Row():
                submit_btn = gr.Button("å‘é€")
                clear_btn = gr.Button("æ¸…é™¤å¯¹è¯")

            # ç»‘å®šå¯¹è¯äº‹ä»¶
            msg.submit(chat_with_pdf, [msg, chatbot], [chatbot, msg])
            submit_btn.click(chat_with_pdf, [msg, chatbot], [chatbot, msg])
            clear_btn.click(lambda: [], None, chatbot)

        with gr.Tab("âš™ï¸ ç³»ç»ŸçŠ¶æ€"):
            status_output = gr.Markdown()
            refresh_btn = gr.Button("åˆ·æ–°çŠ¶æ€")

            refresh_btn.click(get_system_status, None, status_output)
            demo.load(get_system_status, None, status_output)

except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")

    # åˆ›å»ºç®€åŒ–ç‰ˆç•Œé¢
    with gr.Blocks(title="Web RAG ç³»ç»Ÿ") as demo:
        gr.Markdown("# âŒ ä¾èµ–ç¼ºå¤±")
        gr.Markdown(f"**é”™è¯¯**: {e}")
        gr.Markdown("**è§£å†³æ–¹æ¡ˆ**: è¯·è¿è¡Œ `pip3 install langchain langchain-google-genai langchain-community chromadb`")

if __name__ == "__main__":
    try:
        print("ğŸš€ å¯åŠ¨ Web RAG ç³»ç»Ÿ...")
        print(f"ğŸ“‹ API å¯†é’¥çŠ¶æ€: {'âœ… å·²é…ç½®' if os.getenv('GOOGLE_API_KEY') else 'âŒ æœªé…ç½®'}")

        # æ£€æµ‹è¿è¡Œç¯å¢ƒ
        is_spaces = os.getenv("SPACE_ID") is not None

        if is_spaces:
            # Hugging Face Spaces ç¯å¢ƒé…ç½®
            demo.launch(share=True)
        else:
            # æœ¬åœ°å¼€å‘ç¯å¢ƒé…ç½®
            demo.launch(
                server_name="127.0.0.1",
                server_port=7860,
                share=False,
                show_error=True,
                inbrowser=False,
                debug=True  # å¯ç”¨è°ƒè¯•æ¨¡å¼
            )
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")