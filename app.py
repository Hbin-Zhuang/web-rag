#!/usr/bin/env python3
"""
Web RAG ç³»ç»Ÿ - ä¸»ç¨‹åº
åŸºäº Google Gemini çš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ
"""

import gradio as gr
import os
import sys
import tempfile
import traceback
from pathlib import Path

# å¯¼å…¥é…ç½®ç®¡ç†
from config import Config

# éªŒè¯é…ç½®
if not Config.validate_config():
    print("âŒ é…ç½®é”™è¯¯ï¼šè¯·è®¾ç½®GOOGLE_API_KEYç¯å¢ƒå˜é‡")
    print("ğŸ’¡ æç¤ºï¼š")
    print("   1. å¤åˆ¶ .env.example ä¸º .env")
    print("   2. åœ¨ .env æ–‡ä»¶ä¸­å¡«å…¥æ‚¨çš„ Google API Key")
    print("   3. æˆ–è®¾ç½®ç¯å¢ƒå˜é‡: export GOOGLE_API_KEY=your_key_here")
    print("   4. è·å–API Key: https://aistudio.google.com/")
    sys.exit(1)

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä»é…ç½®ä¸­è¯»å–ï¼‰
os.environ["GOOGLE_API_KEY"] = Config.GOOGLE_API_KEY

try:
    from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
    from langchain_community.document_loaders import PyPDFLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.vectorstores import Chroma
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate

    # å¯ç”¨çš„ Gemini æ¨¡å‹åˆ—è¡¨
    AVAILABLE_MODELS = [
        "gemini-2.5-flash-preview-05-20",
        "gemini-2.0-flash",
        "gemini-2.0-flash-lite",
        "gemini-1.5-flash",
        "gemini-1.5-pro",
    ]

    # é»˜è®¤æ¨¡å‹ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    DEFAULT_MODEL = "gemini-2.5-flash-preview-05-20"

    # å…¨å±€å˜é‡
    vectorstore = None
    qa_chain = None
    current_model = DEFAULT_MODEL  # åˆå§‹åŒ–ä¸ºé»˜è®¤æ¨¡å‹
    uploaded_files = []  # è®°å½•å·²ä¸Šä¼ çš„æ–‡ä»¶ä¿¡æ¯

    def create_llm(selected_model=None):
        """åˆ›å»º LLMï¼Œæ”¯æŒæŒ‡å®šæ¨¡å‹æˆ–è‡ªåŠ¨é€‰æ‹©"""
        global current_model

        if selected_model:
            # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹
            model_names = [selected_model]
        else:
            # ä½¿ç”¨é»˜è®¤çš„æ¨¡å‹ä¼˜å…ˆçº§åˆ—è¡¨
            model_names = AVAILABLE_MODELS.copy()

        for model_name in model_names:
            try:
                print(f"å°è¯•æ¨¡å‹: {model_name}")
                llm = ChatGoogleGenerativeAI(
                    model=model_name,
                    temperature=0.3,
                    convert_system_message_to_human=True
                )
                # æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
                test_response = llm.invoke("æµ‹è¯•")
                print(f"âœ… æˆåŠŸä½¿ç”¨æ¨¡å‹: {model_name}")
                current_model = model_name
                return llm
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                if selected_model:
                    # å¦‚æœæŒ‡å®šçš„æ¨¡å‹å¤±è´¥ï¼Œå°è¯•é»˜è®¤æ¨¡å‹
                    print(f"æŒ‡å®šæ¨¡å‹å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹")
                    return create_llm()
                continue

        raise Exception("æ‰€æœ‰ Gemini æ¨¡å‹éƒ½ä¸å¯ç”¨")

    def process_pdf(file):
        """å¤„ç† PDF æ–‡ä»¶å¹¶åˆ›å»ºå‘é‡æ•°æ®åº“"""
        global vectorstore, qa_chain, uploaded_files

        print(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file}")

        if file is None:
            return "âŒ è¯·é€‰æ‹©ä¸€ä¸ª PDF æ–‡ä»¶"

        try:
            # è·å–æ–‡ä»¶è·¯å¾„
            if hasattr(file, 'name'):
                file_path = file.name
                file_name = Path(file_path).name
            else:
                file_path = str(file)
                file_name = Path(file_path).name

            print(f"æ–‡ä»¶è·¯å¾„: {file_path}")
            print(f"æ–‡ä»¶å: {file_name}")

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                return f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"

            # ğŸ”„ æ³¨æ„ï¼šç°åœ¨æ”¯æŒå¤šæ–‡æ¡£ç´¯ç§¯ï¼Œä¸å†é‡ç½®å‘é‡æ•°æ®åº“
            print("æ­£åœ¨å‡†å¤‡å¤„ç†æ–°æ–‡æ¡£...")

            # ä¸å†é‡ç½®å…¨å±€å˜é‡ï¼Œä¿æŒå¤šæ–‡æ¡£ç´¯ç§¯
            # vectorstore = None  # ä¿ç•™ç°æœ‰å‘é‡æ•°æ®åº“
            # qa_chain = None     # ä¿ç•™ç°æœ‰QAé“¾
            # ä¸æ¸…ç©ºæ–‡ä»¶åˆ—è¡¨ï¼Œæ”¯æŒå¤šæ–‡æ¡£ç´¯ç§¯

            print("âœ… ç³»ç»Ÿå‡†å¤‡å®Œæˆï¼Œå°†æ·»åŠ æ–°æ–‡æ¡£åˆ°ç°æœ‰çŸ¥è¯†åº“")

            # åŠ è½½ PDF
            print("æ­£åœ¨åŠ è½½ PDF...")
            loader = PyPDFLoader(file_path)
            documents = loader.load()

            if not documents:
                return "âŒ PDF æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–"

            print(f"æˆåŠŸåŠ è½½ {len(documents)} é¡µæ–‡æ¡£")

            # åˆ†å‰²æ–‡æ¡£
            print("æ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
            )
            texts = text_splitter.split_documents(documents)
            print(f"æ–‡æ¡£åˆ†å‰²ä¸º {len(texts)} ä¸ªç‰‡æ®µ")

            # åˆ›å»ºåµŒå…¥ï¼ˆå¢åŠ è¶…æ—¶æ—¶é—´å’Œé‡è¯•æœºåˆ¶ï¼‰
            print("æ­£åœ¨åˆ›å»ºåµŒå…¥...")
            try:
                embeddings = GoogleGenerativeAIEmbeddings(
                    model="models/embedding-001",
                    request_timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°120ç§’
                )
                print("âœ… Embedding æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ Embedding æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ¡ˆ: {e}")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ä¸åŒçš„ embedding æ¨¡å‹
                try:
                    embeddings = GoogleGenerativeAIEmbeddings(
                        model="models/text-embedding-004",
                        request_timeout=120
                    )
                    print("âœ… ä½¿ç”¨å¤‡ç”¨ embedding æ¨¡å‹æˆåŠŸ")
                except Exception as e2:
                    return f"âŒ æ— æ³•åˆå§‹åŒ– embedding æ¨¡å‹: {str(e2)}"

            # ğŸ”„ æ£€æŸ¥æ˜¯å¦å·²æœ‰å‘é‡æ•°æ®åº“ï¼Œæ”¯æŒå¤šæ–‡æ¡£ç´¯ç§¯
            print("æ­£åœ¨å¤„ç†å‘é‡æ•°æ®åº“...")

            # å¦‚æœå·²å­˜åœ¨å‘é‡æ•°æ®åº“ï¼Œåˆ™æ·»åŠ æ–°æ–‡æ¡£ï¼›å¦åˆ™åˆ›å»ºæ–°çš„
            if vectorstore is not None:
                print("æ£€æµ‹åˆ°å·²æœ‰å‘é‡æ•°æ®åº“ï¼Œå°†æ·»åŠ æ–°æ–‡æ¡£...")
                # å‘ç°æœ‰å‘é‡æ•°æ®åº“æ·»åŠ æ–°æ–‡æ¡£
                try:
                    vectorstore.add_documents(texts)
                    print("âœ… æ–°æ–‡æ¡£å·²æ·»åŠ åˆ°ç°æœ‰å‘é‡æ•°æ®åº“")
                except Exception as e:
                    print(f"âŒ æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
                    # å¦‚æœæ·»åŠ å¤±è´¥ï¼Œé‡æ–°åˆ›å»ºæ•´ä¸ªå‘é‡æ•°æ®åº“
                    print("æ­£åœ¨é‡æ–°åˆ›å»ºå‘é‡æ•°æ®åº“...")
                    vectorstore = Chroma.from_documents(
                        documents=texts,
                        embedding=embeddings
                    )
                    print("âœ… å‘é‡æ•°æ®åº“é‡æ–°åˆ›å»ºæˆåŠŸ")
            else:
                print("åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
                # åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆåˆ†æ‰¹å¤„ç†é¿å…è¶…æ—¶ï¼‰
                try:
                    # åˆ†æ‰¹å¤„ç†å¤§æ–‡æ¡£ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†è¿‡å¤šå†…å®¹å¯¼è‡´è¶…æ—¶
                    batch_size = 10  # æ¯æ‰¹å¤„ç†10ä¸ªæ–‡æ¡£ç‰‡æ®µ
                    if len(texts) > batch_size:
                        print(f"æ–‡æ¡£è¾ƒå¤§ï¼Œå°†åˆ† {(len(texts) + batch_size - 1) // batch_size} æ‰¹å¤„ç†...")

                    vectorstore = Chroma.from_documents(
                        documents=texts,
                        embedding=embeddings
                        # ä½¿ç”¨å†…å­˜æ¨¡å¼ï¼Œé¿å…æ–‡ä»¶æƒé™é—®é¢˜
                    )
                    print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ")
                except Exception as e:
                    print(f"âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥: {e}")
                    if "timeout" in str(e).lower():
                        return f"âŒ ç½‘ç»œè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•: {str(e)}"
                    else:
                        return f"âŒ å‘é‡æ•°æ®åº“åˆ›å»ºå¤±è´¥: {str(e)}"

            # åˆ›å»º QA é“¾
            print("æ­£åœ¨åˆå§‹åŒ– QA é“¾...")
            # ç¡®ä¿ä½¿ç”¨å½“å‰é€‰ä¸­çš„æ¨¡å‹
            llm = create_llm(current_model)

            # è‡ªå®šä¹‰æç¤ºæ¨¡æ¿
            prompt_template = """
            åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚

            ä¸Šä¸‹æ–‡ï¼š
            {context}

            é—®é¢˜ï¼š{question}

            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š
            """

            PROMPT = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )

            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True
            )

            print("QA é“¾åˆå§‹åŒ–æˆåŠŸ")

            # è®°å½•æ–‡ä»¶ä¿¡æ¯
            from datetime import datetime
            file_info = {
                'name': file_name,
                'upload_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'pages': len(documents),
                'chunks': len(texts),
                'model': current_model
            }
            uploaded_files.append(file_info)
            print(f"å·²è®°å½•æ–‡ä»¶ä¿¡æ¯: {file_name}")

            result_message = f"""âœ… æˆåŠŸå¤„ç† PDF æ–‡ä»¶: {file_name}
ğŸ“„ å…±å¤„ç† {len(documents)} é¡µæ–‡æ¡£
ğŸ” åˆ†å‰²ä¸º {len(texts)} ä¸ªæ–‡æ¡£ç‰‡æ®µ
ğŸ“š å·²æ·»åŠ åˆ°çŸ¥è¯†åº“ï¼ˆæ”¯æŒå¤šæ–‡æ¡£ç´¯ç§¯ï¼Œå½“å‰å…± {len(uploaded_files)} ä¸ªæ–‡æ¡£ï¼‰
ğŸ’¾ å†…å­˜å‘é‡æ•°æ®åº“å·²æ›´æ–°ï¼ˆé¿å…æƒé™é—®é¢˜ï¼‰
ğŸ¤– QA é“¾å·²åˆå§‹åŒ–ï¼ˆæ¨¡å‹: {current_model if current_model else 'æœªçŸ¥'}ï¼‰
ğŸ’¡ ç°åœ¨å¯ä»¥å‘æ‰€æœ‰å·²ä¸Šä¼ çš„æ–‡æ¡£æé—®äº†ï¼

ğŸ”„ ç³»ç»ŸçŠ¶æ€å·²æ›´æ–°ï¼Œè¯·æŸ¥çœ‹"ç³»ç»ŸçŠ¶æ€"æ ‡ç­¾é¡µç¡®è®¤"""

            print("å¤„ç†å®Œæˆ")
            return result_message

        except Exception as e:
            error_msg = f"âŒ å¤„ç† PDF æ—¶å‡ºé”™: {str(e)}"
            print(f"é”™è¯¯: {e}")
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return error_msg

    def process_pdf_and_update_status(file, selected_model):
        """å¤„ç† PDF å¹¶æ›´æ–°æ¨¡å‹çŠ¶æ€"""
        global current_model

        # å…ˆæ›´æ–°å½“å‰æ¨¡å‹
        current_model = selected_model

        # å¤„ç† PDF
        result = process_pdf(file)

        # è¿”å›å¤„ç†ç»“æœã€æ›´æ–°çš„æ¨¡å‹çŠ¶æ€ã€ç³»ç»ŸçŠ¶æ€å’Œæ–‡ä»¶åˆ—è¡¨
        model_status_text = f"å½“å‰æ¨¡å‹: {current_model}\nçŠ¶æ€: å·²å°±ç»ª\n\nğŸ’¡ æç¤º: æ–‡æ¡£å·²åŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¯¹è¯"
        system_status_text = get_system_status()
        files_display = get_uploaded_files_display()

        return result, model_status_text, system_status_text, files_display

    def chat_with_pdf(message, history):
        """ä¸ PDF å†…å®¹å¯¹è¯"""
        global qa_chain

        print(f"æ”¶åˆ°æ¶ˆæ¯: {message}")

        if not message.strip():
            return history, ""

        if qa_chain is None:
            response = "âŒ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç† PDF æ–‡ä»¶"
            history.append([message, response])
            return history, ""

        try:
            print("æ­£åœ¨æŸ¥è¯¢ QA é“¾...")
            # æŸ¥è¯¢ QA é“¾
            result = qa_chain({"query": message})
            response = result["result"]

            print(f"è·å¾—å›ç­”: {response[:100]}...")

            # æ·»åŠ æºæ–‡æ¡£ä¿¡æ¯
            if "source_documents" in result and result["source_documents"]:
                sources = set()
                for doc in result["source_documents"]:
                    if hasattr(doc, 'metadata') and 'source' in doc.metadata:
                        sources.add(Path(doc.metadata['source']).name)

                if sources:
                    response += f"\n\nğŸ“š å‚è€ƒæ¥æº: {', '.join(sources)}"

            history.append([message, response])
            return history, ""

        except Exception as e:
            error_response = f"âŒ æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
            print(f"æŸ¥è¯¢é”™è¯¯: {e}")
            print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            history.append([message, error_response])
            return history, ""

    def switch_model(selected_model):
        """åˆ‡æ¢æ¨¡å‹"""
        global qa_chain, current_model

        # æ›´æ–°å½“å‰æ¨¡å‹
        current_model = selected_model

        if not vectorstore:
            return f"âŒ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç† PDF æ–‡ä»¶ï¼Œç„¶åå†åˆ‡æ¢æ¨¡å‹", current_model

        # ä¿å­˜å½“å‰æ¨¡å‹ä½œä¸ºå¤‡ä»½
        previous_model = current_model

        try:
            print(f"æ­£åœ¨åˆ‡æ¢åˆ°æ¨¡å‹: {selected_model}")
            llm = create_llm(selected_model)

            # é‡æ–°åˆ›å»º QA é“¾
            prompt_template = """
            åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯´"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚

            ä¸Šä¸‹æ–‡ï¼š
            {context}

            é—®é¢˜ï¼š{question}

            è¯·ç”¨ä¸­æ–‡å›ç­”ï¼š
            """

            PROMPT = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question"]
            )

            qa_chain = RetrievalQA.from_chain_type(
                llm=llm,
                chain_type="stuff",
                retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
                chain_type_kwargs={"prompt": PROMPT},
                return_source_documents=True
            )

            success_message = f"""âœ… æ¨¡å‹åˆ‡æ¢æˆåŠŸ

å½“å‰æ¨¡å‹: {current_model}
çŠ¶æ€: å·²å°±ç»ª

ğŸ’¡ æç¤º: å¦‚æœå·²ä¸Šä¼ æ–‡æ¡£ï¼Œå¯ä»¥ç›´æ¥å¼€å§‹å¯¹è¯"""

            return success_message, current_model

        except Exception as e:
            error_message = f"âŒ åˆ‡æ¢æ¨¡å‹å¤±è´¥: {str(e)}"
            print(f"æ¨¡å‹åˆ‡æ¢å¤±è´¥ï¼Œå›é€€åˆ°: {previous_model}")
            # å›é€€æ¨¡å‹çŠ¶æ€
            current_model = previous_model
            # è¿”å›é”™è¯¯æ¶ˆæ¯å’Œå›é€€çš„æ¨¡å‹
            return error_message, previous_model

    def get_system_status():
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        global vectorstore, qa_chain, current_model, uploaded_files

        # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
        vectorstore_status = "âŒ æœªåŠ è½½"
        if vectorstore is not None:
            try:
                # å°è¯•è·å–æ–‡æ¡£æ•°é‡æ¥éªŒè¯å‘é‡æ•°æ®åº“æ˜¯å¦æ­£å¸¸
                doc_count = len(vectorstore.get()['documents']) if hasattr(vectorstore, 'get') else "æ— æ³•è·å–"
                vectorstore_status = f"âœ… å·²åŠ è½½ (æ–‡æ¡£æ•°: {doc_count})"
            except:
                vectorstore_status = "âœ… å·²åŠ è½½"

        # æ£€æŸ¥QAé“¾çŠ¶æ€
        qa_status = "âŒ æœªåˆå§‹åŒ–"
        if qa_chain is not None:
            qa_status = "âœ… å·²åˆå§‹åŒ–"

        # æ£€æŸ¥å½“å‰æ¨¡å‹çŠ¶æ€
        model_status = current_model if current_model else "æœªåˆå§‹åŒ–"

        status = f"""
## ğŸ“Š ç³»ç»ŸçŠ¶æ€

**Python ç‰ˆæœ¬**: {sys.version.split()[0]}

**å·¥ä½œç›®å½•**: {os.getcwd()}

**API å¯†é’¥**: {'âœ… å·²é…ç½®' if os.getenv('GOOGLE_API_KEY') else 'âŒ æœªé…ç½®'}

**å½“å‰æ¨¡å‹**: {model_status}

**å‘é‡æ•°æ®åº“**: {vectorstore_status}

**QA é“¾**: {qa_status}

---

## ğŸ“‹ ä½¿ç”¨è¯´æ˜

1. åœ¨"æ–‡æ¡£ä¸Šä¼ "æ ‡ç­¾é¡µä¸Šä¼  PDF æ–‡ä»¶
2. ç­‰å¾…å¤„ç†å®Œæˆï¼ˆæŸ¥çœ‹çŠ¶æ€ä¿¡æ¯ï¼‰
3. åœ¨"æ™ºèƒ½å¯¹è¯"æ ‡ç­¾é¡µæé—®
4. ç³»ç»Ÿä¼šåŸºäºæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜

---

## ğŸ”§ æŠ€æœ¯æ ˆ

**LLM**: Google Gemini (è‡ªåŠ¨é€‰æ‹©å¯ç”¨æ¨¡å‹)

**åµŒå…¥æ¨¡å‹**: Google Embedding-001

**å‘é‡æ•°æ®åº“**: ChromaDB

**æ¡†æ¶**: LangChain + Gradio

---

## ğŸ› è°ƒè¯•ä¿¡æ¯

**å‘é‡å­˜å‚¨å¯¹è±¡**: {type(vectorstore).__name__ if vectorstore else 'None'}

**QA é“¾å¯¹è±¡**: {type(qa_chain).__name__ if qa_chain else 'None'}

---

## ğŸš€ æ”¯æŒçš„ Gemini æ¨¡å‹

**æœ€æ–° 2.5 ç³»åˆ— (Preview)**
- `gemini-2.5-flash-preview-05-20` - æœ€æ–° Flashï¼Œæ”¯æŒæ€ç»´é“¾æ¨ç†

**ç¨³å®š 2.0 ç³»åˆ—**
- `gemini-2.0-flash` - ä¸‹ä¸€ä»£ç‰¹æ€§ï¼Œç”Ÿäº§ç¯å¢ƒæ¨è
- `gemini-2.0-flash-lite` - æˆæœ¬ä¼˜åŒ–ç‰ˆï¼Œé«˜é¢‘è°ƒç”¨

**å¤‡ç”¨ 1.5 ç³»åˆ—**
- `gemini-1.5-flash` - å¿«é€Ÿå¤šæ¨¡æ€å¤„ç†
- `gemini-1.5-pro` - å¤æ‚æ¨ç†ä»»åŠ¡

---

## ğŸ’¡ æ¨¡å‹é€‰æ‹©ç­–ç•¥

ç³»ç»Ÿä¼šè‡ªåŠ¨æŒ‰ä¼˜å…ˆçº§å°è¯•æ¨¡å‹ï¼š
1. **ä¼˜å…ˆ**: æœ€æ–° 2.5 ç³»åˆ—ï¼ˆæ€§èƒ½æœ€ä½³ï¼‰
2. **å¤‡é€‰**: ç¨³å®š 2.0 ç³»åˆ—ï¼ˆç”Ÿäº§å¯é ï¼‰
3. **å…œåº•**: 1.5 ç³»åˆ—ï¼ˆç¡®ä¿å¯ç”¨æ€§ï¼‰

---

"""

        return status

    def get_uploaded_files_display():
        """è·å–å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨çš„æ˜¾ç¤ºå†…å®¹"""
        global uploaded_files

        if not uploaded_files:
            return "*æš‚æ— å·²ä¸Šä¼ æ–‡ä»¶*"

        files_display = "## ğŸ“„ å·²ä¸Šä¼ æ–‡ä»¶\n\n"
        for i, file_info in enumerate(uploaded_files, 1):
            files_display += f"""**{i}. {file_info['name']}**
- ğŸ“… ä¸Šä¼ æ—¶é—´: {file_info['upload_time']}
- ğŸ“‘ é¡µæ•°: {file_info['pages']} é¡µ
- ğŸ” æ–‡æ¡£ç‰‡æ®µ: {file_info['chunks']} ä¸ª
- ğŸ¤– ä½¿ç”¨æ¨¡å‹: {file_info['model']}

"""
        return files_display

    # åˆ›å»º Gradio ç•Œé¢
    with gr.Blocks(title="Web RAG ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸš€ Web RAG ç³»ç»Ÿ")
        gr.Markdown("åŸºäº Google Gemini çš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")

        with gr.Tab("ğŸ“„ æ–‡æ¡£ä¸Šä¼ "):
            gr.Markdown("### ä¸Šä¼  PDF æ–‡æ¡£")
            gr.Markdown("**æ³¨æ„**: ä¸Šä¼ åè¯·ç­‰å¾…å¤„ç†å®Œæˆï¼ŒçŠ¶æ€ä¼šæ˜¾ç¤ºåœ¨ä¸‹æ–¹")

            with gr.Row():
                with gr.Column(scale=2):
                    file_input = gr.File(
                        label="é€‰æ‹© PDF æ–‡ä»¶",
                        file_types=[".pdf"]
                    )
                    upload_output = gr.Textbox(
                        label="å¤„ç†çŠ¶æ€",
                        lines=6,
                        interactive=False,
                        placeholder="ç­‰å¾…æ–‡ä»¶ä¸Šä¼ ..."
                    )

                    # å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨
                    uploaded_files_display = gr.Markdown(
                        label="å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨",
                        value="*æš‚æ— å·²ä¸Šä¼ æ–‡ä»¶*"
                    )

                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ¤– æ¨¡å‹é…ç½®")
                    model_dropdown = gr.Dropdown(
                        choices=AVAILABLE_MODELS,
                        value=DEFAULT_MODEL,
                        label="é€‰æ‹© Gemini æ¨¡å‹",
                        info="é€‰æ‹©åè‡ªåŠ¨åˆ‡æ¢æ¨¡å‹"
                    )
                    model_status = gr.Textbox(
                        label="æ¨¡å‹çŠ¶æ€",
                        value=f"å½“å‰æ¨¡å‹: {DEFAULT_MODEL}\nçŠ¶æ€: å·²å°±ç»ª",
                        interactive=False,
                        lines=3
                    )

        with gr.Tab("ğŸ’¬ æ™ºèƒ½å¯¹è¯"):
            gr.Markdown("### ä¸æ–‡æ¡£å†…å®¹å¯¹è¯")
            gr.Markdown("**æç¤º**: è¯·å…ˆä¸Šä¼ å¹¶å¤„ç† PDF æ–‡ä»¶ï¼Œç„¶ååœ¨æ­¤æé—®")

            chatbot = gr.Chatbot()
            msg = gr.Textbox()
            with gr.Row():
                submit_btn = gr.Button("å‘é€")
                clear_btn = gr.Button("æ¸…é™¤å¯¹è¯")

            # ç»‘å®šå¯¹è¯äº‹ä»¶
            msg.submit(chat_with_pdf, [msg, chatbot], [chatbot, msg])
            submit_btn.click(chat_with_pdf, [msg, chatbot], [chatbot, msg])
            clear_btn.click(lambda: [], None, chatbot)

        with gr.Tab("âš™ï¸ ç³»ç»ŸçŠ¶æ€"):
            with gr.Row():
                with gr.Column(scale=2):
                    status_output = gr.Markdown(
                        value=get_system_status(),
                        label="ç³»ç»ŸçŠ¶æ€"
                    )
                    refresh_btn = gr.Button("ğŸ”„ åˆ·æ–°çŠ¶æ€", variant="secondary")

                    # åˆ·æ–°æŒ‰é’®äº‹ä»¶
                    refresh_btn.click(
                        fn=get_system_status,
                        outputs=status_output
                    )

                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨")
                    models_info = gr.Markdown(f"""
**é»˜è®¤æ¨¡å‹**: `{DEFAULT_MODEL}`

**æ‰€æœ‰å¯ç”¨æ¨¡å‹**:
{chr(10).join([f'- `{model}`' for model in AVAILABLE_MODELS])}

**æ¨¡å‹è¯´æ˜**:
- **2.5 ç³»åˆ—**: æœ€æ–°é¢„è§ˆç‰ˆï¼Œæ€§èƒ½æœ€ä½³
- **2.0 ç³»åˆ—**: ç¨³å®šç‰ˆï¼Œç”Ÿäº§æ¨è
- **1.5 ç³»åˆ—**: å¤‡ç”¨ç‰ˆï¼Œç¡®ä¿å¯ç”¨æ€§
                    """)

        # äº‹ä»¶ç»‘å®š - åœ¨æ‰€æœ‰ç»„ä»¶å®šä¹‰å®Œæˆå
        # æ–‡ä»¶ä¸Šä¼ äº‹ä»¶
        file_input.upload(
            fn=process_pdf_and_update_status,
            inputs=[file_input, model_dropdown],
            outputs=[upload_output, model_status, status_output, uploaded_files_display]
        )

        # æ–‡ä»¶æ¸…é™¤æ—¶é‡ç½®çŠ¶æ€
        file_input.clear(
            fn=lambda: "ç­‰å¾…æ–‡ä»¶ä¸Šä¼ ...",
            inputs=None,
            outputs=upload_output
        )

        # æ¨¡å‹ä¸‹æ‹‰æ¡†æ”¹å˜æ—¶è‡ªåŠ¨åˆ‡æ¢
        model_dropdown.change(
            fn=switch_model,
            inputs=model_dropdown,
            outputs=[model_status, model_dropdown]  # åŒæ—¶æ›´æ–°çŠ¶æ€å’Œä¸‹æ‹‰æ¡†å€¼
        )

except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")

    # åˆ›å»ºç®€åŒ–ç‰ˆç•Œé¢
    with gr.Blocks(title="Web RAG ç³»ç»Ÿ") as demo:
        gr.Markdown("# âŒ ä¾èµ–ç¼ºå¤±")
        gr.Markdown(f"**é”™è¯¯**: {e}")
        gr.Markdown("**è§£å†³æ–¹æ¡ˆ**: è¯·è¿è¡Œ `pip3 install langchain langchain-google-genai langchain-community chromadb`")

if __name__ == "__main__":
    try:
        print("ğŸš€ å¯åŠ¨ Web RAG ç³»ç»Ÿ...")
        print(f"ğŸ“‹ API å¯†é’¥çŠ¶æ€: {'âœ… å·²é…ç½®' if os.getenv('GOOGLE_API_KEY') else 'âŒ æœªé…ç½®'}")

        # æ£€æµ‹è¿è¡Œç¯å¢ƒ
        is_spaces = os.getenv("SPACE_ID") is not None

        if is_spaces:
            # Hugging Face Spaces ç¯å¢ƒé…ç½®
            demo.launch(share=True)
        else:
            # æœ¬åœ°å¼€å‘ç¯å¢ƒé…ç½®
            demo.launch(
                server_name="127.0.0.1",
                server_port=7860,
                share=False,
                show_error=True,
                inbrowser=False,
                debug=True  # å¯ç”¨è°ƒè¯•æ¨¡å¼
            )
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")